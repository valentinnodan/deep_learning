{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "927e43dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a85ebec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-31 03:50:36.783935: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-31 03:50:36.783956: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Reusing dataset sms_spam (/home/max/.cache/huggingface/datasets/sms_spam/plain_text/1.0.0/53f051d3b5f62d99d61792c91acefe4f1577ad3e4c216fb0ad39e30b9f20019c)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33b3b8392ca744e189d3eed941327ad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "dataset = load_dataset('sms_spam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6510e70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sms', 'label'],\n",
       "    num_rows: 5574\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02a63c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import vocab\n",
    "\n",
    "PAD_WORD = '<pad>'\n",
    "\n",
    "def get_vocab(X):\n",
    "    return vocab.build_vocab_from_iterator([sentence.split() for sentence in X], specials=[\"<unk>\", \"<pad>\"])\n",
    "\n",
    "def pad(seq, size):\n",
    "    if len(seq) < size:\n",
    "        seq = seq + [PAD_WORD] * (size - len(seq))\n",
    "    return seq\n",
    "\n",
    "def to_indices(vocab, words):\n",
    "    stoi = vocab.get_stoi()\n",
    "    return [stoi[w] for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce85db50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sms', 'label'],\n",
       "        num_rows: 5574\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9960fc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut = 1000\n",
    "\n",
    "from random import sample\n",
    "\n",
    "newA = [(a, b) for a, b in zip(dataset['train']['sms'], dataset['train']['label'])]\n",
    "\n",
    "newA = sample(newA, cut)\n",
    "\n",
    "\n",
    "newB = [(a, b) for a, b in zip(dataset['train']['sms'], dataset['train']['label'])]\n",
    "\n",
    "newB = sample(newB, cut)\n",
    "\n",
    "\n",
    "X_train = []\n",
    "Y_train = []\n",
    "for a, b in newA:\n",
    "    X_train.append(a)\n",
    "    Y_train.append(b)\n",
    "\n",
    "X_test = []\n",
    "Y_test = []\n",
    "for a, b in newB:\n",
    "    X_test.append(a)\n",
    "    Y_test.append(b)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "823f2043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "589"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen = 0\n",
    "for str1 in X_train:\n",
    "    maxlen = max(maxlen, len(str1))\n",
    "\n",
    "for str1 in X_test:\n",
    "    maxlen = max(maxlen, len(str1))\n",
    "    \n",
    "maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bea7ef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vocab()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab1 = get_vocab(X_train + X_test)\n",
    "vocab1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f650a313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7626"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(vocab1)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "006214ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert = BertModel.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e39b6ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53a0f473",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "import enum\n",
    "\n",
    "MAX_SEQ = 32\n",
    "\n",
    "class Model(enum.Enum):\n",
    "    BERT = 3\n",
    "    DISTIL_BILSTM = 2\n",
    "    CLASSIC_BILSTM = 1\n",
    "\n",
    "def bert_token(X, y, tokenizer):\n",
    "    lines = [\" \".join(s) for s in X]\n",
    "    masks = [[int(word != PAD_WORD) for word in sentence] for sentence in X]\n",
    "    inds = torch.tensor([tokenizer.encode(line.split(), add_special_tokens=False) for line in lines])\n",
    "    masks = torch.tensor(masks, dtype=torch.int8)\n",
    "    torch_y = torch.tensor(y, dtype=torch.float)\n",
    "    return TensorDataset(inds, torch_y, masks)\n",
    "\n",
    "def bilstm_token(X, y, vocab, teacher_output, model):\n",
    "    torch_x = torch.tensor(X, dtype=torch.long)\n",
    "    torch_y = torch.tensor(y, dtype=torch.float)\n",
    "\n",
    "    if model == Model.DISTIL_BILSTM:\n",
    "        return TensorDataset(torch_x, torch_y, teacher_output)\n",
    "    else:\n",
    "        return TensorDataset(torch_x, torch_y)\n",
    "\n",
    "def tokenize(X, y, model = Model.CLASSIC_BILSTM, vocab=None, teacher_output = None, tokenizer = None):    \n",
    "    X_padded = [pad(s, MAX_SEQ) for s in [t.split()[:MAX_SEQ] for t in X]]\n",
    "    \n",
    "    if model == Model.BERT:\n",
    "        return bert_token(X_padded, y, tokenizer)\n",
    "    else:\n",
    "        return bilstm_token([to_indices(vocab, s) for s in X_padded], y, vocab, teacher_output, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "921f4c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TeacherModel(nn.Module):\n",
    "    def init(self, teacher):\n",
    "        super(TeacherModel, self).init()\n",
    "        self.teacher = teacher\n",
    "        self.fc1 = nn.Linear(self.teacher.config.hidden_size, 2)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, inp):\n",
    "        inds, labels, masks = inp\n",
    "        labels = torch.tensor(labels, dtype=torch.long)\n",
    "        output = self.teacher(inds, attention_mask=masks)[0][:, 0, :]\n",
    "        pred = self.fc1(output)\n",
    "        loss = self.loss(pred, labels)\n",
    "        return loss, pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "999cc5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = TeacherModel(bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa6ca783",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bert_dataset_train = tokenize(X_train, Y_train, tokenizer=tokenizer, model=Model.BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fcad2efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_dataset_test = tokenize(X_test, Y_test, tokenizer=tokenizer, model=Model.BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1762fd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "batch_size = 8\n",
    "\n",
    "def train(model, dataset, epochs=5):\n",
    "    dataloader = DataLoader(dataset, batch_size, shuffle=True)\n",
    "    optimizer = Adam(model.parameters())\n",
    "    model.train()\n",
    "    for e in range(epochs):\n",
    "        print(\"epoch\", e)\n",
    "        losses = 0\n",
    "        count = 0\n",
    "        for info in tqdm(dataloader):\n",
    "            loss, _ = model(info)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses += loss\n",
    "            count += 1\n",
    "        losses /= count\n",
    "        print(f'Loss: {losses.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ead73f3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                               | 0/125 [00:00<?, ?it/s]/tmp/ipykernel_70710/289778998.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.long)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [02:41<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 \t| Loss: 0.32838818430900574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [03:02<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \t| Loss: 0.27588585019111633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [02:29<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 \t| Loss: 0.2665775418281555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [02:52<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 \t| Loss: 0.23004834353923798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [02:18<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 \t| Loss: 0.21688580513000488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:48<00:00,  2.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.8899999856948853\n"
     ]
    }
   ],
   "source": [
    "train(bert, bert_dataset_train, bert_dataset_test, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9a7c5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_output(modell, dataset):\n",
    "    dataloader = DataLoader(dataset, batch_size)\n",
    "    output = []\n",
    "    with torch.no_grad():\n",
    "        for info in tqdm(dataloader):\n",
    "            loss, result = modell(info)\n",
    "            output.append(result)\n",
    "        output = torch.cat(output)\n",
    "    return output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af67aebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "loaded_bert_model = pickle.load(open(\"bert.model\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e91e8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                               | 0/125 [00:00<?, ?it/s]/tmp/ipykernel_86841/289778998.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.long)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:42<00:00,  2.96it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:49<00:00,  2.52it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bert_output_train = get_output(loaded_bert_model, bert_dataset_train)\n",
    "bert_output_test = get_output(loaded_bert_model, bert_dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68737941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.909"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "sklearn.metrics.accuracy_score(Y_test, torch.argmax(bert_output_test, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "caadd859",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "distil_dataset_train = tokenize(X_train, Y_train, teacher_output=bert_output_train, vocab=vocab1, model=Model.DISTIL_BILSTM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eaa0ed3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "distil_dataset_test = tokenize(X_test, Y_test, teacher_output=bert_output_test, vocab=vocab1, model=Model.DISTIL_BILSTM)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bad6c5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DistilModel(nn.Module):\n",
    "    def __init__(self, student, alpha=0.75):\n",
    "        super(DistilModel, self).__init__()\n",
    "        self.student = student\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def loss(self, real_prediction, real_output, teacher_prediction, teacher_output):\n",
    "        bce = nn.CrossEntropyLoss()\n",
    "        mse = nn.MSELoss()\n",
    "        prediction_loss = bce(real_prediction, torch.tensor(real_output, dtype=torch.long))\n",
    "        teacher_loss = mse(teacher_prediction, teacher_output)\n",
    "        return self.alpha * prediction_loss + (1 - self.alpha) * teacher_loss\n",
    "\n",
    "    def forward(self, inp):\n",
    "        inds, labels, teacher_output = inp\n",
    "        labels = torch.tensor(labels, dtype=torch.long)\n",
    "        label_prediction = self.student(inds)\n",
    "        loss = self.loss(label_prediction, labels, label_prediction, teacher_output)\n",
    "        return loss, label_prediction\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, bilstm):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.bilstm = bilstm\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, inp):\n",
    "        inds, labels = inp\n",
    "        labels = torch.tensor(labels, dtype=torch.long)\n",
    "        prediction = self.bilstm(inds)\n",
    "        loss = self.loss(prediction, labels)\n",
    "        return loss, prediction\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "88a473b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, n_layers,\n",
    "                 bidirectional, dropout, device=None):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "\n",
    "        self.rnn = nn.LSTM(embedding_dim,\n",
    "                           hidden_dim,\n",
    "                           num_layers=n_layers,\n",
    "                           bidirectional=bidirectional,\n",
    "                           dropout=dropout)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def init_state(self, batch_size):\n",
    "        return torch.zeros(2 * self.num_layers, batch_size, self.hidden_dim), \\\n",
    "               torch.zeros(2 * self.num_layers, batch_size, self.hidden_dim)\n",
    "\n",
    "    def forward(self, text, text_lengths=None):\n",
    "        x = self.embedding(text)\n",
    "        x = torch.transpose(x, dim0=1, dim1=0)\n",
    "        x, self.hidden = self.rnn(x)\n",
    "        hidden, cell = self.hidden\n",
    "        hidden = self.dropout(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1))\n",
    "        x = self.fc(hidden)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "136c06c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/max/.local/lib/python3.10/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "distil_model = SimpleLSTM(\n",
    "            input_dim=len(vocab1),\n",
    "            embedding_dim=16,\n",
    "            hidden_dim=16,\n",
    "            output_dim=2,\n",
    "            n_layers=1,\n",
    "            bidirectional=True,\n",
    "            dropout=0.5)\n",
    "\n",
    "student = DistilModel(distil_model, alpha=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f7b499c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                               | 0/125 [00:00<?, ?it/s]/tmp/ipykernel_86841/1172701668.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.long)\n",
      "/tmp/ipykernel_86841/1172701668.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  prediction_loss = bce(real_prediction, torch.tensor(real_output, dtype=torch.long))\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:01<00:00, 85.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.58984637260437\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:01<00:00, 80.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.455444574356079\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:01<00:00, 64.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.2421942949295044\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:02<00:00, 61.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0976306200027466\n",
      "epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:02<00:00, 60.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0756251811981201\n",
      "epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:01<00:00, 64.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0726757049560547\n",
      "epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:02<00:00, 61.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.9582139253616333\n",
      "epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:02<00:00, 61.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8994405269622803\n",
      "epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:02<00:00, 59.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8417882323265076\n",
      "epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:02<00:00, 57.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8652071356773376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train(student, distil_dataset_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "905f4de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                               | 0/125 [00:00<?, ?it/s]/tmp/ipykernel_86841/1172701668.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.long)\n",
      "/tmp/ipykernel_86841/1172701668.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  prediction_loss = bce(real_prediction, torch.tensor(real_output, dtype=torch.long))\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:00<00:00, 300.29it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.896"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.accuracy_score(Y_test, torch.argmax(get_output(student, distil_dataset_test), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c940c075",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/max/.local/lib/python3.10/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "base_model = SimpleLSTM(input_dim=vocab_size,\n",
    "               embedding_dim=16,\n",
    "               hidden_dim=16,\n",
    "               output_dim=2,\n",
    "               n_layers=1,\n",
    "               bidirectional=True,\n",
    "               dropout=0.8)\n",
    "\n",
    "basic_model = SimpleModel(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4573288e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "basic_dataset_train = tokenize(X_train, Y_train, vocab=vocab1, model=Model.CLASSIC_BILSTM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f0aed12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "basic_dataset_test = tokenize(X_test, Y_test, vocab=vocab1, model=Model.CLASSIC_BILSTM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d51cc02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                               | 0/125 [00:00<?, ?it/s]/tmp/ipykernel_86841/1172701668.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.long)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:01<00:00, 64.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.4701578915119171\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:02<00:00, 62.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.34898239374160767\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:02<00:00, 61.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.29840487241744995\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:01<00:00, 66.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.26334959268569946\n",
      "epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:01<00:00, 66.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.22582900524139404\n",
      "epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:01<00:00, 66.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.20650890469551086\n",
      "epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:01<00:00, 67.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.1654800921678543\n",
      "epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:02<00:00, 55.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.14020206034183502\n",
      "epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:02<00:00, 59.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.1163373813033104\n",
      "epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:02<00:00, 56.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.13328538835048676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train(basic_model, basic_dataset_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2771ed05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                               | 0/125 [00:00<?, ?it/s]/tmp/ipykernel_86841/1172701668.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.long)\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:00<00:00, 281.72it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.912"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.accuracy_score(Y_test, torch.argmax(get_output(basic_model, basic_dataset_test), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889c5a63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
