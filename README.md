# deep-learning

В ДЗ4 на собственном горьком опыте убедилась в том, что BERT - очень медленная и прожорливая в смысле памяти штука. 

Изначально пыталась как в статье для imdb делать, но это совершенно убийственно, поменяла датасет на смски, т.к. они маленькие и для классификации на 2 класса можно сделать датасет поменьше, а это очень критично было, т.к. компьютер именно по памяти умирал.

В результате минимизировала количество слов (то, после которого идет PAD_WORD), брала рандомный срез у датасета и в качестве теста, и в качестве трейна (чтобы уменьшить расход памяти на хранение датасета). Как-то обучила, но модель получилась довольно игрушечная, конечно, скорее всего, на полноценном датасете более репрезентативно было бы
